{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baaeedfb",
   "metadata": {},
   "source": [
    "CUE Conflict Implementation from https://github.com/irasin/Pytorch_AdaIN/blob/master/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0cf671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import vgg19, VGG19_Weights\n",
    "import random\n",
    "import os\n",
    "import torchvision\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a503282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean_std(features):\n",
    "    \"\"\"\n",
    "\n",
    "    :param features: shape of features -> [batch_size, c, h, w]\n",
    "    :return: features_mean, feature_s: shape of mean/std ->[batch_size, c, 1, 1]\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size, c = features.size()[:2]\n",
    "    features_mean = features.reshape(batch_size, c, -1).mean(dim=2).reshape(batch_size, c, 1, 1)\n",
    "    features_std = features.reshape(batch_size, c, -1).std(dim=2).reshape(batch_size, c, 1, 1) + 1e-6\n",
    "    return features_mean, features_std\n",
    "\n",
    "\n",
    "def adain(content_features, style_features):\n",
    "    \"\"\"\n",
    "    Adaptive Instance Normalization\n",
    "\n",
    "    :param content_features: shape -> [batch_size, c, h, w]\n",
    "    :param style_features: shape -> [batch_size, c, h, w]\n",
    "    :return: normalized_features shape -> [batch_size, c, h, w]\n",
    "    \"\"\"\n",
    "    content_mean, content_std = calc_mean_std(content_features)\n",
    "    style_mean, style_std = calc_mean_std(style_features)\n",
    "    normalized_features = style_std * (content_features - content_mean) / content_std + style_mean\n",
    "    return normalized_features\n",
    "\n",
    "\n",
    "class VGGEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vgg = vgg19(weights=VGG19_Weights.IMAGENET1K_V1).features\n",
    "        self.slice1 = vgg[: 2]\n",
    "        self.slice2 = vgg[2: 7]\n",
    "        self.slice3 = vgg[7: 12]\n",
    "        self.slice4 = vgg[12: 21]\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, images, output_last_feature=False):\n",
    "        h1 = self.slice1(images)\n",
    "        h2 = self.slice2(h1)\n",
    "        h3 = self.slice3(h2)\n",
    "        h4 = self.slice4(h3)\n",
    "        if output_last_feature:\n",
    "            return h4\n",
    "        else:\n",
    "            return h1, h2, h3, h4\n",
    "\n",
    "\n",
    "class RC(nn.Module):\n",
    "    \"\"\"A wrapper of ReflectionPad2d and Conv2d\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, pad_size=1, activated=True):\n",
    "        super().__init__()\n",
    "        self.pad = nn.ReflectionPad2d((pad_size, pad_size, pad_size, pad_size))\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "        self.activated = activated\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.pad(x)\n",
    "        h = self.conv(h)\n",
    "        if self.activated:\n",
    "            return F.relu(h)\n",
    "        else:\n",
    "            return h\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rc1 = RC(512, 256, 3, 1)\n",
    "        self.rc2 = RC(256, 256, 3, 1)\n",
    "        self.rc3 = RC(256, 256, 3, 1)\n",
    "        self.rc4 = RC(256, 256, 3, 1)\n",
    "        self.rc5 = RC(256, 128, 3, 1)\n",
    "        self.rc6 = RC(128, 128, 3, 1)\n",
    "        self.rc7 = RC(128, 64, 3, 1)\n",
    "        self.rc8 = RC(64, 64, 3, 1)\n",
    "        self.rc9 = RC(64, 3, 3, 1, False)\n",
    "\n",
    "    def forward(self, features):\n",
    "        h = self.rc1(features)\n",
    "        h = F.interpolate(h, scale_factor=2)\n",
    "        h = self.rc2(h)\n",
    "        h = self.rc3(h)\n",
    "        h = self.rc4(h)\n",
    "        h = self.rc5(h)\n",
    "        h = F.interpolate(h, scale_factor=2)\n",
    "        h = self.rc6(h)\n",
    "        h = self.rc7(h)\n",
    "        h = F.interpolate(h, scale_factor=2)\n",
    "        h = self.rc8(h)\n",
    "        h = self.rc9(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vgg_encoder = VGGEncoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def generate(self, content_images, style_images, alpha=1.0):\n",
    "        content_features = self.vgg_encoder(content_images, output_last_feature=True)\n",
    "        style_features = self.vgg_encoder(style_images, output_last_feature=True)\n",
    "        t = adain(content_features, style_features)\n",
    "        t = alpha * t + (1 - alpha) * content_features\n",
    "        out = self.decoder(t)\n",
    "        return out\n",
    "\n",
    "    def calc_content_loss(out_features, t):\n",
    "        return F.mse_loss(out_features, t)\n",
    "\n",
    "    def calc_style_loss(content_middle_features, style_middle_features):\n",
    "        loss = 0\n",
    "        for c, s in zip(content_middle_features, style_middle_features):\n",
    "            c_mean, c_std = calc_mean_std(c)\n",
    "            s_mean, s_std = calc_mean_std(s)\n",
    "            loss += F.mse_loss(c_mean, s_mean) + F.mse_loss(c_std, s_std)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, content_images, style_images, alpha=1.0, lam=10):\n",
    "        content_features = self.vgg_encoder(content_images, output_last_feature=True)\n",
    "        style_features = self.vgg_encoder(style_images, output_last_feature=True)\n",
    "        t = adain(content_features, style_features)\n",
    "        t = alpha * t + (1 - alpha) * content_features\n",
    "        out = self.decoder(t)\n",
    "\n",
    "        output_features = self.vgg_encoder(out, output_last_feature=True)\n",
    "        output_middle_features = self.vgg_encoder(out, output_last_feature=False)\n",
    "        style_middle_features = self.vgg_encoder(style_images, output_last_feature=False)\n",
    "\n",
    "        loss_c = self.calc_content_loss(output_features, t)\n",
    "        loss_s = self.calc_style_loss(output_middle_features, style_middle_features)\n",
    "        loss = loss_c + lam * loss_s\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f3c337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cue-conflict dataset created at ./cue_conflict_dataset\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "# Transforms\n",
    "cue_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "to_pil = torchvision.transforms.ToPILImage()\n",
    "\n",
    "# STL-10 test dataset\n",
    "cue_test_dataset = torchvision.datasets.STL10(root=\"./data/stl\", train=False, download=False, transform=cue_transform)\n",
    "cue_test_loader = torch.utils.data.DataLoader(cue_test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Custom style dataset\n",
    "class StyleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, style_folder, transform=None):\n",
    "        self.files = [os.path.join(style_folder, f)\n",
    "                      for f in os.listdir(style_folder)\n",
    "                      if f.lower().endswith(('png','jpg','jpeg'))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.files[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "style_dataset = StyleDataset(\"./data/styles\", transform=cue_transform)\n",
    "\n",
    "# Load pretrained model\n",
    "model = Model()\n",
    "state_dict = torch.load(\"model_state.pth\", map_location=torch.device('cpu'))\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Output folder\n",
    "output_root = \"cue_conflict_dataset\"\n",
    "os.makedirs(output_root, exist_ok=True)\n",
    "\n",
    "# Generate cue-conflict images\n",
    "with torch.no_grad():\n",
    "    for i, (content_batch, labels) in enumerate(cue_test_loader):\n",
    "        content_batch = content_batch.to(device)\n",
    "\n",
    "        # pick a random style for this batch\n",
    "        style_img = random.choice(style_dataset).unsqueeze(0).to(device)\n",
    "        style_batch = style_img.expand(content_batch.size(0), -1, -1, -1)\n",
    "\n",
    "        # generate cue-conflict outputs\n",
    "        outputs = model.generate(content_batch, style_batch, alpha=0.1)\n",
    "\n",
    "        # save each output in a subfolder with its label\n",
    "        for j, out_tensor in enumerate(outputs):\n",
    "            out_tensor = out_tensor.cpu().clamp(0,1)\n",
    "            out_img = to_pil(out_tensor)\n",
    "            label = labels[j].item()\n",
    "            \n",
    "            # create subfolder for this label\n",
    "            label_folder = os.path.join(output_root, str(label))\n",
    "            os.makedirs(label_folder, exist_ok=True)\n",
    "            \n",
    "            # save image\n",
    "            filename = os.path.join(label_folder, f\"content_{i*cue_test_loader.batch_size + j}.png\")\n",
    "            out_img.save(filename)\n",
    "\n",
    "print(\"Cue-conflict dataset created at ./cue_conflict_dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
